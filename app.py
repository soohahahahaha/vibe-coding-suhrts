import streamlit as st
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from PyPDF2 import PdfReader
import os

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="PDF ì±—ë´‡", page_icon="ğŸ“š", layout="wide")

# API í‚¤ ì„¤ì •
GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]

# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_from_pdf(pdf_file):
    text = ""
    pdf_reader = PdfReader(pdf_file)
    for page in pdf_reader.pages:
        text += page.extract_text()
    return text

# ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
@st.cache_resource
def create_vectorstore(text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=GEMINI_API_KEY
    )
    
    vectorstore = FAISS.from_texts(chunks, embeddings)
    return vectorstore

# ëŒ€í™” ì²´ì¸ ìƒì„±
def create_conversation_chain(vectorstore):
    # í™˜ê° ë°©ì§€ í”„ë¡¬í”„íŠ¸
    prompt_template = """ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì¤‘ìš”í•œ ê·œì¹™:
1. ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ(Context)ì˜ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì— ëŒ€í•œ ì§ˆë¬¸ì„ ë°›ìœ¼ë©´ "ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
3. ì¶”ì¸¡í•˜ê±°ë‚˜ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
4. í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  ì†”ì§íˆ ë§í•˜ì„¸ìš”.

Context: {context}

Chat History: {chat_history}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

    PROMPT = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "chat_history", "question"]
    )
    
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp",
        google_api_key=GEMINI_API_KEY,
        temperature=0.1,  # ë‚®ì€ temperatureë¡œ í™˜ê° ë°©ì§€
        convert_system_message_to_human=True
    )
    
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )
    
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 4}  # ë” ë§ì€ ë¬¸ë§¥ ì œê³µ
        ),
        memory=memory,
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": PROMPT}
    )
    
    return conversation_chain

# ë©”ì¸ UI
st.title("ğŸ“š PDF ê¸°ë°˜ AI ì±—ë´‡")
st.markdown("**Gemini 2.0 Flash** ëª¨ë¸ | ë¬¸ì„œ ë‚´ìš©ë§Œ ì°¸ì¡°í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤")

# ì‚¬ì´ë“œë°” - íŒŒì¼ ì—…ë¡œë“œ
with st.sidebar:
    st.header("ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ")
    
    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=["pdf"],
        help="ìµœëŒ€ 200MBê¹Œì§€ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤"
    )
    
    use_default = st.checkbox("ê¸°ë³¸ test.pdf ì‚¬ìš©", value=False)
    
    if st.button("ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘", type="primary"):
        with st.spinner("ë¬¸ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                # íŒŒì¼ ì„ íƒ
                if use_default and os.path.exists("test.pdf"):
                    pdf_file = open("test.pdf", "rb")
                elif uploaded_file:
                    pdf_file = uploaded_file
                else:
                    st.error("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê¸°ë³¸ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”")
                    st.stop()
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = extract_text_from_pdf(pdf_file)
                
                if len(text) < 100:
                    st.error("PDFì—ì„œ ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    st.stop()
                
                # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
                vectorstore = create_vectorstore(text)
                st.session_state.vectorstore = vectorstore
                st.session_state.conversation = create_conversation_chain(vectorstore)
                st.session_state.messages = []
                
                st.success(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ! ({len(text):,}ì)")
                
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”"):
        if "messages" in st.session_state:
            st.session_state.messages = []
            if "conversation" in st.session_state:
                st.session_state.conversation.memory.clear()
            st.rerun()

# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ë©”ì‹œì§€ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
    if "conversation" not in st.session_state:
        st.warning("âš ï¸ ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•˜ì„¸ìš”")
        st.stop()
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                response = st.session_state.conversation({"question": prompt})
                answer = response["answer"]
                
                st.markdown(answer)
                
                # ì°¸ì¡° ë¬¸ì„œ í‘œì‹œ
                if response.get("source_documents"):
                    with st.expander("ğŸ“– ì°¸ì¡°í•œ ë¬¸ì„œ ë¶€ë¶„ ë³´ê¸°"):
                        for i, doc in enumerate(response["source_documents"][:3]):
                            st.markdown(f"**ì°¸ì¡° {i+1}:**")
                            st.text(doc.page_content[:400] + "...")
                            st.markdown("---")
                
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
            except Exception as e:
                error_msg = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})

# í•˜ë‹¨ ì •ë³´
st.sidebar.markdown("---")
st.sidebar.info("""
**ì‚¬ìš© ë°©ë²•:**
1. PDF íŒŒì¼ ì—…ë¡œë“œ ë˜ëŠ” test.pdf ì„ íƒ
2. 'ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘' ë²„íŠ¼ í´ë¦­
3. ì±„íŒ…ì°½ì—ì„œ ì§ˆë¬¸ ì…ë ¥

**íŠ¹ì§•:**
- ë¬¸ì„œ ë‚´ìš©ë§Œ ì°¸ì¡°í•˜ì—¬ ë‹µë³€
- ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” "ëª¨ë¥¸ë‹¤"ê³  ë‹µë³€
- í™˜ê°(Hallucination) ë°©ì§€

**ëª¨ë¸:** Gemini 2.0 Flash Experimental
""")

# ì˜ˆì‹œ ì§ˆë¬¸
if "conversation" in st.session_state and len(st.session_state.messages) == 0:
    st.markdown("### ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸")
    st.markdown("""
    - ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”
    - [íŠ¹ì • ì£¼ì œ]ì— ëŒ€í•´ ë¬¸ì„œì—ì„œ ë¬´ì—‡ì´ë¼ê³  ë§í•˜ë‚˜ìš”?
    - ë¬¸ì„œì—ì„œ ì–¸ê¸‰ëœ í•µì‹¬ í‚¤ì›Œë“œëŠ” ë¬´ì—‡ì¸ê°€ìš”?
    """)
